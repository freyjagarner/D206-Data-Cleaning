{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed91689-7c94-422d-a8c5-5033525768b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb72a9f-1e81-40cb-a882-07108db22808",
   "metadata": {},
   "source": [
    "Chelsey De Dios\n",
    "\n",
    "D206 Data Cleaning\n",
    "\n",
    "Keiona Middleton\n",
    "\n",
    "4 November 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b57c2-99d7-477c-a545-3f3e2f03ed41",
   "metadata": {},
   "source": [
    "# Part I**: Research Question\n",
    "\n",
    "## A.  Describe one question or decision that you will address using the data set you chose. The summarized question or decision must be relevant to a realistic organizational need or situation.\n",
    "\n",
    "The research question in this case would be can we predict customer churn rate based on customer data.\n",
    " \n",
    "## B.  Describe the variables in the data set and indicate the specific type of data being described. Use examples from the data set that support your claims.\n",
    "\n",
    "The variable descriptions are given in the file accompanying the data and are as follows:\n",
    "* **CaseOrder**: A placeholder variable to preserve the original order of the raw data file\n",
    "* **Customer_id****: Unique customer ID \n",
    "* **Interaction**: Unique IDs related to customer transactions, technical support, and sign-ups\n",
    "* **City**: Customer city of residence as listed on the billing statement\n",
    "* **State**: Customer state of residence as listed on the billing statement\n",
    "* **County**: Customer county of residence as listed on the billing statement\n",
    "* **Zip**: Customer zip code of residence as listed on the billing statement\n",
    "* **Lat, Lng**: GPS coordinates of customer residence as listed on the billing statement \n",
    "* **Population**: Population within a mile radius of customer, based on census data\n",
    "* **Area**: Area type (rural, urban, suburban), based on census data\n",
    "* **TimeZone**: Time zone of customer residence based on customer’s sign-up information\n",
    "* **Job**: Job of the customer (or invoiced person) as reported in sign-up information\n",
    "* **Children**: Number of children in customer’s household as reported in sign-up information \n",
    "* **Age**: Age of customer as reported in sign-up information\n",
    "* **Education**: Highest degree earned by customer as reported in sign-up information \n",
    "* **Employment**: Employment status of customer as reported in sign-up information\n",
    "* **Income**: Annual income of customer as reported at time of sign-up\n",
    "* **Marital**: Marital status of customer as reported in sign-up information\n",
    "* **Gender**: Customer self-identification as male, female, or nonbinary\n",
    "* **Churn**: Whether the customer discontinued service within the last month (yes, no) \n",
    "* **Outage_sec_perweek**: Average number of seconds per week of system outages in the customer’s neighborhood\n",
    "* **Email**: Number of emails sent to the customer in the last year (marketing or correspondence)\n",
    "* **Contacts**: Number of times customer contacted technical support\n",
    "* **Yearly_equip_failure**: The number of times customer’s equipment failed and had to be reset/replaced in the past year\n",
    "* **Techie**: Whether the customer considers themselves technically inclined (based on customer questionnaire when they signed up for services) (yes, no)\n",
    "* **Contract**: The contract term of the customer (month-to-month, one year, two year)\n",
    "* **Port_modem**: Whether the customer has a portable modem (yes, no)\n",
    "* **Tablet**: Whether the customer owns a tablet such as iPad, Surface, etc. (yes, no) \n",
    "* **InternetService**: Customer’s internet service provider (DSL, fiber optic, None)\n",
    "* **Phone**: Whether the customer has a phone service (yes, no)\n",
    "* **Multiple**: Whether the customer has multiple lines (yes, no)\n",
    "* **OnlineSecurity**: Whether the customer has an online security add-on (yes, no) \n",
    "* **OnlineBackup**: Whether the customer has an online backup add-on (yes, no) \n",
    "* **DeviceProtection**: Whether the customer has device protection add-on (yes, no) \n",
    "* **TechSupport**: Whether the customer has a technical support add-on (yes, no)\n",
    "* **StreamingTV**: Whether the customer has streaming TV (yes, no)\n",
    "* **StreamingMovies**: Whether the customer has streaming movies (yes, no)\n",
    "* **PaperlessBilling**: Whether the customer has paperless billing (yes, no)\n",
    "* **PaymentMethod**: The customer’s payment method (electronic check, mailed check, bank (automatic bank transfer), credit card (automatic))\n",
    "* **Tenure**: Number of months the customer has stayed with the provider\n",
    "* **MonthlyCharge**: The amount charged to the customer monthly. This value reflects an average per customer.\n",
    "* **Bandwidth_GB_Year**: The average amount of data used, in GB, in a year by the customer \n",
    "\n",
    "The following variables represent responses to an eight-question survey asking customers to rate the importance of various factors/surfaces on a scale of 1 to 8 (1 = most important, 8 = least important)\n",
    "\n",
    "   * **Item1**: Timely response\n",
    "   * **Item2**: Timely fixes\n",
    "   * **Item3**: Timely replacements\n",
    "   * **Item4**: Reliability\n",
    "   * **Item5**: Options\n",
    "   * **Item6**: Respectful response\n",
    "   * **Item7**: Courteous exchange\n",
    "   * **Item8**: Evidence of active listening\n",
    " \n",
    "# Part II: Data-Cleaning Plan\n",
    " \n",
    "## C.  Explain the plan for cleaning the data by doing the following:\n",
    "   ### 1.  Propose a plan that includes the relevant techniques and specific steps needed to identify anomalies in the data set.\n",
    "    \n",
    "   I will clean the data using these steps:\n",
    "   1. Import the data using pandas and Python into a Jupyter Notebook File\n",
    "   2. Look at the shape and head of the file to see the amount of data we are dealing with and the gain information about the data.\n",
    "   3. Drop any unnecessary variables and rename any improperly named variables.\n",
    "   4. Deal with any obviously improperly formatted data.\n",
    "   5. Look for and deal with duplicate data.\n",
    "   6. Look for and deal with Null Data.\n",
    "   7. Convert datatypes to appropriate types for that column.\n",
    "   8. Look at the unique values in the columns searching for inconsistencies and unlikely data.\n",
    "   9. Look at the statistical information for numerical data to search for inconsistences and outliers or unlikely data.\n",
    "   \n",
    "### 2.  Justify your approach for assessing the quality of the data, include:\n",
    "\n",
    "   *  characteristics of the data being assessed.\n",
    "   *  the approach used to assess the quality.\n",
    "### 3.  Justify your selected programming language and any libraries and packages that will support the data-cleaning process.\n",
    "\n",
    "I chose Python since it has such a rich collection of libraries for data analysis.\n",
    "I used pandas to work with dataframes which allows for a great deal of data manipulation and analysis.\n",
    "I used numpy to add random values and to work with pandas.\n",
    "I used matplotlib and seaborn for plotting graphs to make data insights.\n",
    "\n",
    "### 4.  Provide the code you will use to identify the anomalies in the data.\n",
    "\n",
    "The code is included in a seperate .ipynb file.\n",
    "\n",
    "# Part III: Data Cleaning\n",
    "\n",
    "## D.  Summarize the data-cleaning process by doing the following:\n",
    "    \n",
    "   ### 1.  Describe the findings, including all anomalies, from the implementation of the data-cleaning plan from part C.\n",
    "    \n",
    "   ### 2.  Justify your methods for mitigating each type of discovered anomaly in the data set.\n",
    "   ### 3.  Summarize the outcome from the implementation of each data-cleaning step.\n",
    "   ### 4.  Provide the code used to mitigate anomalies.**\n",
    "\n",
    "- First, it was useful to drop the duplicate index column which was unnamed, since it added no data. This removed the unnecessary column.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fafa1c-d6e9-4456-8067-f4ca42d0f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate CaseOrder column\n",
    "df = df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41ac00-24b3-4c3f-b93d-28a75e09c9d7",
   "metadata": {
    "tags": [
     "hide-output",
     "remove-output"
    ]
   },
   "source": [
    "- Next it was useful to rename the columns item1-item8 to their values to make them more descriptive. This resulted in having properly named columns so as to not have to keep referring back to the data documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a347b-47ea-4e6b-ad7d-333cbf681611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of current column names mapping to desired column names\n",
    "survey_dict = {'item1':'timely_responses', 'item2':'timely_fixes', 'item3':'timely_replacements', \n",
    "               'item4':'reliability', 'item5':'options', 'item6':'respectful_response', \n",
    "               'item7':'courteous_exchange', 'item8':'evidence_of_active_listening'}\n",
    "\n",
    "# rename the column names based on survey_dict\n",
    "df = df.rename(columns=survey_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecb77f-594f-4e0c-9899-3cb507dd0e40",
   "metadata": {},
   "source": [
    "- Next we looked at improperly formatted data, which was only the zip codes which were missing their leading zeros. These had to be added back to have the true zip code of the area. With this step, we gained fully formatted zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a6a17-2dc8-4b5a-bf7e-d33326b77c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an empty dataframe to store short zip codes\n",
    "incorrect_zips = pd.DataFrame()\n",
    "\n",
    "# get a list of all unique zip codes\n",
    "zips = list(df['Zip'].unique())\n",
    "\n",
    "# append rows containing short zip codes to incorrect_zips dataframe\n",
    "for i in zips:\n",
    "    if len(str(i)) < 5:\n",
    "        incorrect_zips = incorrect_zips.append(df.loc[df['Zip'] == i])\n",
    "\n",
    "# function to fill zip codes less than 5 numbers long with leading zeros\n",
    "def fill_zeros(x):\n",
    "    if len(x) < 6:\n",
    "        return x.zfill(5)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# convert zip code type to string to add zeros\n",
    "df.Zip = df.Zip.astype(str)\n",
    "\n",
    "# apply zero filling function to zip codes.\n",
    "df.Zip = df.Zip.apply(fill_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914b8b5-37c6-45c4-a3a6-2a8c7f6ce5dc",
   "metadata": {},
   "source": [
    "- There was no improper duplicate data, and so there was nothing to fix with duplicate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62c09f-6b27-4ced-849e-0fc58d51b632",
   "metadata": {},
   "source": [
    " - This dataset contains information on only 10,000 customers. Thousands of customers in this had null data for at least one variable and so these null values had to be logically dealt with instead of being dropped.\n",
    " \n",
    "* For null children I randomly assigned 0 or 1 child to the households with missing data. Over half of customers had one of these values for the number of children with 0 being the most common value and 1 being the median value, and so these were randomly assigned. This resulted in the loss of null values and a similar distribution of children values that we had to be gin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc806a-7481-477f-b6b8-c9afdce3cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the NA values of children to 0 or 1 randomly\n",
    "df.loc[df['Children'].isna(), 'Children'] = np.random.choice([0,1], len(df.loc[df['Children'].isna(), 'Children']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03a2fb-7bcd-4323-8cae-4056d1b5f45e",
   "metadata": {},
   "source": [
    "* For null ages, it seemed that the ages were fairly evenly distributed from 18-89, and there seemed to be no relationship to the customer age and other customer data. Due to this, ages from 18-89 were randomly assigned to customers missing age data. This resulted in a similar age distribution to what we started with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3348e-3f5a-4367-a386-ba6cfa35f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random ages in age range\n",
    "age_range = np.arange(start=18, stop=89)\n",
    "\n",
    "# assign random ages in age range to NA ages in our data\n",
    "df.loc[df.Age.isna(), 'Age'] = np.random.choice(age_range, len(df.loc[df.Age.isna(), 'Age']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d473086-0408-47cd-bbde-a252d21c7187",
   "metadata": {},
   "source": [
    "* For null value in Techie, Phone and TechSupport I assigned Yes and No values based on the current distribution of yes and no values for customers that we did have data for to maintain the integrity of these values. This resulted in a near exact distribution to what we began with in these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6accc36-d04f-47ce-b20a-524aacb91b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where Techie is NA and assign a random Yes or No value based on the current distribution of Yes and No values\n",
    "df.loc[df['Techie'].isna(), 'Techie'] = np.random.choice([\"Yes\", \"No\"], len(df.loc[df['Techie'].isna(), 'Techie']), p=[0.167088, .832912])\n",
    "\n",
    "# find where Phone is NA and assign a random Yes or No value based on the current distribution of Yes and No values\n",
    "df.loc[df['Phone'].isna(), 'Phone'] = np.random.choice(['Yes', 'No'], len(df.loc[df['Phone'].isna(), 'Phone']), p=[.91, 0.09])\n",
    "\n",
    "# find where TechSupport is NA and assign a random Yes or No value based on the current distribution of Yes and No values\n",
    "df.loc[df['TechSupport'].isna(), 'TechSupport'] = np.random.choice(['Yes', 'No'], len(df.loc[df['TechSupport'].isna(), 'TechSupport']), p=[0.374514,0.625486])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb55393-c755-4b8e-8dc5-932128ab7401",
   "metadata": {},
   "source": [
    "* For Income, Tenure and Bandwith per Year I added the median values, since the minimum and maximum values were so far from each other as to nearly be outliers, which can hurt the integrity of the mean value. This resulted in a similar income distribution as the true values in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e61ca-47db-4350-8f2b-e0cdd19445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the median income to NA incomes\n",
    "df.loc[df['Income'].isna(), 'Income'] = df['Income'].median()\n",
    "\n",
    "# assign the median Tenure to NA Tenures\n",
    "df.loc[df['Tenure'].isna(), 'Tenure'] = df['Tenure'].median()\n",
    "\n",
    "# assign the median Bandwidth_GB_Year to NA Bandwidth_GB_Year\n",
    "df.loc[df['Bandwidth_GB_Year'].isna(), 'Bandwidth_GB_Year'] = df['Bandwidth_GB_Year'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a8252-3b96-434f-9c35-10ba51946bd5",
   "metadata": {},
   "source": [
    "* For the datatypes I converted them to the best type for the data that they contained. This resulted in properly typed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605c588-b23d-4711-a65c-1556608b4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dataframe columns to more appropriate data types\n",
    "df = df.astype( {'Customer_id':'string', 'Interaction':'string', 'City':'string', 'State':'string', 'County':'string', 'Zip':'string', 'Lat':float,\n",
    "       'Lng':float, 'Population':int, 'Area':'category', 'Timezone':'category', 'Job':'category', 'Children':int, 'Age':int,\n",
    "       'Education':'category', 'Employment':'category', 'Income':float, 'Marital':'category', 'Gender':'category', 'Churn':'category',\n",
    "       'Outage_sec_perweek':float, 'Email':int, 'Contacts':int, 'Yearly_equip_failure':int,\n",
    "       'Techie':'category', 'Contract':'category', 'Port_modem':'category', 'Tablet':'category', 'InternetService':'category',\n",
    "       'Phone':'category', 'Multiple':'category', 'OnlineSecurity':'category', 'OnlineBackup':'category',\n",
    "       'DeviceProtection':'category', 'TechSupport':'category', 'StreamingTV':'category', 'StreamingMovies':'category',\n",
    "       'PaperlessBilling':'category', 'PaymentMethod':'category', 'Tenure':float, 'MonthlyCharge':float,\n",
    "       'Bandwidth_GB_Year':float, 'timely_responses':int, 'timely_fixes':int, 'timely_replacements':int, 'reliability':int, 'options':int,\n",
    "       'respectful_response':int, 'courteous_exchange':int, 'evidence_of_active_listening':int}, copy=False)\n",
    "\n",
    "# return our new dataframe of datatypes\n",
    "get_dtypes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ce6c81-1113-4a2f-af09-b4ccf1184ad5",
   "metadata": {},
   "source": [
    "* Looking through the unique string and categorical data I saw nothing that needed to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1019f1b3-f81d-4dad-b9d7-61d39de93cc3",
   "metadata": {},
   "source": [
    "* In the numerical data it was found that the minimum population was zero, which was problematic since at least the customer had to live where they said they did. To fill these zeros, I found the mean population for all customers from the type of area the customer lived in. I then found the mean of this data and added that for the zero in population. I used the mean in this case, since there was not necessarily much area data to go on, so the median might have been a less correct assumption. This slightly brought up the distribution of children in the 0-1 range, but resulted in overall similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5755b-7595-46e8-8d15-9616a6223025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all possible areas\n",
    "areas = df['Area'].unique()\n",
    "\n",
    "# assign the mean populations in each area to the area in a dictionary\n",
    "area_dict = {}\n",
    "for i in areas:\n",
    "    area_dict[i] = df.loc[df['Area'] == i]['Population'].mean()\n",
    "\n",
    "# assign the mean area populations to the missing population values based on their area in the dictionary.\n",
    "for k, v in area_dict.items():\n",
    "    df.loc[(df['Population'] == 0) & (df['Area'] == k), 'Population'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf5fc0-18f5-49dc-b5b9-a6dda0cecee8",
   "metadata": {},
   "source": [
    "* Also in the numerical data, I noticed that the minimum outage seconds per wee was a negative number. It is not possible to have a negative amount of outage seconds for internet, so I converted all of these negative values to zero, which slightly brought up the mean of outage seconds per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2a9a4-f6f5-49b0-ab7d-1b5e9f40be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 0 to negative values for outage seconds per week\n",
    "df.loc[df['Outage_sec_perweek'] < 0, 'Outage_sec_perweek'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f135b-a5e5-4962-8927-80c8372a5656",
   "metadata": {},
   "source": [
    "**5.  Provide a copy of the cleaned data set.**\n",
    "\n",
    "   A copy of the cleaned dataset will be submitted with this report.\n",
    "    \n",
    "**6.  Summarize the limitations of the data-cleaning process.**\n",
    "\n",
    "   The most significant limitations in this data cleaning were that the data was randomized, that there were so many null values, and that there was such little data to work with. The fact that the data was randomly generated, in the customer demographic area specifically, meant that no truly meaningful insights could be made with the data, and making patterns out of common sense was impossible. The fact that there were so many null values to fill, including age and children for customers, when other customer demographic information was inconsistent with the real world meant that creative ways of filling the data were not possible. Much of the data had to be randomized to make it consistent with the complete data in the file. When one has to guess at data completely, it makes it hard to know if it is consistent with the true data. The fact that there were only 10,000 customers and dirty data means that the insights made from the data may be unreliable since it's from such a small sampling.\n",
    "    \n",
    "**7.  Discuss how the limitations in part D6 affect the analysis of the question or decision from part A.**\n",
    "\n",
    "   These limitations mean that the conclusions drawn from this data should be only tentatively accepted and tested before any decisions are made based on this data. It should not be taken, for example, that certain age groups are more likely to churn immediately, since nearly 25% of our ages are guessed at in this analysis. These hypotheses should be tested before any business decitions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781db3b-c1d5-46d9-939c-9a3e8b4f313e",
   "metadata": {},
   "source": [
    "**E.  Apply principal component analysis (PCA) to identify the significant features of the data set by doing the following:**\n",
    "\n",
    "    1.  List the principal components in the data set.\n",
    "    2.  Describe how you identified the principal components of the data set.\n",
    "    3.  Describe how the organization can benefit from the results of the PCA\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882228c-5e64-4c04-86c5-6a66d1333b52",
   "metadata": {},
   "source": [
    "\n",
    "Part IV. Supporting Documents\n",
    "\n",
    "F.  Provide a Panopto recording that demonstrates the warning- and error-free functionality of the code used to support the discovery of anomalies and the data cleaning process and summarizes the programming environment.\n",
    " \n",
    " \n",
    "G.  Reference the web sources used to acquire segments of third-party code to support the application. Be sure the web sources are reliable.\n",
    " No third party code was used.\n",
    " \n",
    "H.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
